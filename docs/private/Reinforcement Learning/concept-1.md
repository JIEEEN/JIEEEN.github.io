---
layout: default
title: Concept-1
nav_order: 2
parent: Reinforcement_Learning
grand_parent: private
use_math: true
---
---
### 목차
- [개념](#개념)
  - [MDP](#mdp)
  - [정책 (policy)](#정책-policy)
  - [궤적 (trajectory)](#궤적-trajectory)
  - [감가율 (discount factor)](#감가율-discount-factor)
  - [가치함수](#가치함수)
    - [상태가치함수](#상태가치함수)
    - [행동가치함수](#행동가치함수)
  - [벨만 방정식](#벨만-방정식)


---

# 개념


## MDP
- __Markov Decision Process__
- 현재 행동이 다음 상태와 결과에 영향을 미치는 순차적 의사결정 문제를 수식으로 표현하는 방법
- 내가 풀고싶은 강화학습 문제를 MDP형태로 표현하는게 우선
- 나아가 Markov Property라는 개념이 존재하는데, 다음 상태를 결정하기 위해서 현재 상태와 행동만 필요할 뿐, 그 이전의 이력은 필요하지 않다는 것이다.
  - 하지만 대부분의 강화학습 문제들이 이 Markov Property를 지키지 않는다. Atari만 봐도, 공의 속도, 방향을 상태 하나만으로 파악하기는 불가능하므로, 그 이전에 발생한 프레임들도 포함해 상태를 정의한다.
  - Bayes 정리와 엮여서 가치함수를 정의하는데도 사용된다.
  
## 정책 (policy)
- 상태를 입력으로 주면 행동을 반환하는 함수
- 결정론적(deterministic), 확룔적(stochastic) 두가지가 존재
  - 결정론적 : $a_t = \eta(s_t)$
  - 확률적 : $a_t$ ~ $\pi(.\|s_t)$
- 예를 들어 아침 7시 기상시간이 돼 누워있는 상태에서 일어나는 행동을 하는 것이 어떤 정책에 따른 것이라고 얘기할 수 있다.
  - 반대로 새벽 3시에 자고 있는 상황에서 일어나는 것은 별로 좋은 행동은 아니다(좋은 정책이 아니다). 이처럼 정책은 에이전트에게 주어진 상태에 따라 달라질 수 있다.

## 궤적 (trajectory)
- MDP에서 정책을 따라 행동할 때 발생하는 상태와 행동의 순서
- $(s_0, a_0, s_1, a_1, s_2, a_2, ...)$
- $\tau$로 표시한다.
- 이러한 궤적에서의 행동의 결과로 받게되는 보상을 더한 것을 $G(\tau)$로 표현한다.
  
## 감가율 (discount factor)
- $\gamma$로 표시하며, 각 행동을 통해 얻는 보상앞에 곱해진다.
- 직관적인 의미로는 __현재의 보상이 미래의 보상보다 가치가 높다__ 이다.
- 대부분의 사람들이 오늘 100만원을 받는것과 내일 100만원을 받는것중 무엇을 선택할까? 당연히 오늘 받는 것이다. 내일 받는 것에는 감가율이 적용돼 오늘 받는 것보다 가치가 줄어든 것이다.
- 감가율을 적용해 보상함수를 다시 작성하면 다음과 같다
- $G(\tau) = r_0 + \gamma r_1 + \gamma ^2 r_2 + ... = \displaystyle\sum_{t=0}^{k}{\gamma ^t r_t}$
- $\gamma$는 다음 몇스텝까지의 보상을 인정해주는지에 관련한 중요한 지표이다.
  - 만약 $\gamma$ = 0.8이라고 가정해보자. 언뜻 크게 인정하는 것처럼 보이지만, 10스텝까지만 가도 $0.8^10$의 값이 0.1정도가 돼 정말 많이 줄어들었음을 알 수 있다.
  - 대부분의 강화학습에서 0.99~0.999의 값을 이용한다고 한다.
    - 대략 몇백스텝까지 인정해준다.
    - Dota2에 적용된 강화학습의 경우 미래의 보상을 80000스텝 이후까지 생각해야한다고 한다.
  - 강화학습을 적용시킬 문제의 보상이 대략 몇스텝일지 파악해 $\gamma$를 조절하는게 중요하다. $\gamma$는 학습 속도와도 연관돼있다.

## 가치함수
- __상태가치함수__ 와 __행동가치함수__, 두가지가 존재한다.
  
### 상태가치함수
- 가치함수라고도 하며 V로 표기
- 상태에 대한 가치를 반환하는 함수
- $V_\pi(s) = E_\pi[G\|s_0 = s] = E_\pi[\displaystyle\sum_{t=0}^{k}{\gamma ^t r_t}\|s_0 = s]$


### 행동가치함수
- 큐함수라고도 하며 Q로 표기
- 상태에서 특정 행동에 대한 가치를 반환하는 함수
- $Q_\pi(s, a) = E_\pi[G\|s_0 = s, a_0 = a] = E_\pi[\displaystyle\sum_{t=0}^{k}{\gamma ^t r_t}\|s_0 = s, a_0 = a]$
- 상태가치함수는 행동가치함수를 이용해 다시 표현할 수 있다.
- $V_\pi(s) = E_\pi[Q_\pi(s, a)]$
  - Q를 G에 대한 기댓값 공식을 이용한다음 Bayes rule을 통해 식을 변환한 후, Markov Property를 적용해 식을 간소화시키면 V를 얻을 수 있다.
  - 위 식을 직관적으로 생각해보면 특정 상태에서의 가치는 그 상태에서 행할 수 있는 모든 행동들의 가치의 평균과도 같다는 것이다.
  - ![image](https://github.com/JIEEEN/JIEEEN.github.io/assets/63636210/be593215-fc5e-4979-bf7c-5d1ebdec02e5){: width="300" height = "300"}
  - 위 그림만 봤을 때는 간단해지긴 했지만, 그만큼 정보가 많이 사라진 느낌이다.

## 벨만 방정식
- 위에서 본 두 함수의 문제점을 찾을 수 있을까? 바로 정말 먼 미래에 해당하는 보상까지 알아야 함수를 계산할 수 있다는 점이다. 이를 해결하기 위한 방법이 __벨만 방정식__ 이다.
- $V_\pi(s) = E_\pi[G_t\|s_0=s] = E_\pi[r_t + \gamma G_{t+1}\|s_0 = s] = E_\pi[r_t + \gamma V_\pi(s_{t+1})\|s_t=s, a_t $ ~ $ \pi(s_t)]$
- $Q_\pi(s, a) = E_\pi[G_t\|s_0=s, a_0=a] = E_\pi[r_t + \gamma G_{t+1}\|s_0 = s, a_0 = a] = E_\pi[r_t + \gamma Q_\pi(s_{t+1}, a_{t+1})\|s_t=s, a_t = a]$
- 벨만 방정식이 적용됨으로써, 먼 미래에 대한 보상까지 구할 필요없이 현재 시점에 획득한 보상과 다음 시점의 가치만으로 V와 Q를 업데이트할 수 있다.
